\documentclass{beamer}
\usepackage{tikz}
\usepackage{float}
\usepackage{graphicx}
\usepackage[most]{tcolorbox}
\usepackage{comment}
\usetikzlibrary{decorations.pathreplacing}
\tcbuselibrary{theorems}
\tcbset{enhanced,colframe=blue!20,colback={black!5!white},drop shadow}
\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]
\usetheme[numbering=fraction]{metropolis}           % Use metropolis theme
\title{Bachelor Thesis Marginal-Sampling}
\date{\today}
\author{Michael Fedders}

\newcommand{\R}{\mathbb{R}}
\newcommand{\dx}{\, \mathrm{d}}
\begin{document}
	\maketitle
  	\begin{frame}[plain]
		\frametitle{Table of Contents}
		\tableofcontents
	\end{frame}
	
\section{Introduction}
	
	\begin{frame}{Model}
		We assume that we can describe a (biological) process through a function
		$x(t, \theta)$ with time $t$ and unknown model parameters $\theta$.
		Through (un)voluntary limitations the measured data is not $x$ but
		\[
			\overline{y} = c + h\bigl(x(t,\theta)\bigr) + \varepsilon
  		\]
		where
  		\begin{itemize}
  		\item $\overline{y}$ is the measured data
  		\item $c$ is an offset parameter
  		\item $h$ is the observation function
  		\item $\varepsilon$ is a noise - for now $\epsilon \sim \mathcal{N}(0, 
  		1/\lambda)$
  		\end{itemize}
	\end{frame}
	
	\begin{frame}{Standard approach}
		The standard approach is to use a data set $D$ to determine the model 
		parameters $\theta$ and the offset $c$ and noise parameter $\lambda$ with 			Bayes theorem:
		\[
			p(\theta,c,\lambda \mid D) = \frac{p(D \mid \theta,c,\lambda) \cdot
			p(\theta, c, \lambda)}{p(D)}.
		\]
		We can then use Markov chain Monte Carlo (MCMC) methods to proportionally 
		sample the posterior distribution with the product of likelihood 
		$p(D \mid \theta, c, \lambda)$ and prior $p(\theta, c, \lambda)$. We will 
		call this way \textbf{FP-approach} from now on
	\end{frame}
	
	\begin{frame}{Hierarchical approach}
		For Maximum Likelihood methods it was shown that it can be faster to first 
		derive the model parameter $\theta$ and then in a second step the noise
		and transformation (e.g. offset, scaling) parameter.
		\begin{itemize}
			\item[$\implies$] We would like to do the same for the posterior
			sampling
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Hierarchical approach}
		For this \textbf{MP-approach} we have to calculate the marginalised 
		likelihood first:
		\begin{align*}
			p(D \mid \theta) &= \iint p(D \mid \theta, c, \lambda) \cdot 
			p(\theta, c, \lambda) \dx c \dx \lambda.
		\end{align*}
	\end{frame}
	
	\begin{frame}{Hierarchical approach}
		For this \textbf{MP-approach} we have to calculate the marginalised 
		likelihood first:
		\begin{align*}
			p(D \mid \theta) &= \iint p(D \mid \theta, c, \lambda) \cdot 
			p(\theta, c, \lambda) \dx c \dx \lambda \\
			&= \iint p(D \mid \theta, c, \lambda) \cdot 
			p(c, \lambda) \dx c \dx \lambda \cdot p(\theta)
		\end{align*}
	\end{frame}
	
	\begin{frame}{Hierarchical approach}
		We can then use that
		\[
			p(\theta \mid D) = \frac{p(D \mid \theta) \cdot
			p(\theta)}{p(D)}
		\]
		to proportionally sample from the marginalized posterior 
		$p(\theta \mid D)$, again with MCMC methods.
	\end{frame}
	
	\begin{frame}{MCMC}
		maybe an extra frame to explain MCMC and Parallel Tempering
	\end{frame}
	
	
	
\section{Gaussian noise}
  	
  	\begin{frame}{Normal-Gamma prior}
  		The normal-gamma prior depends on 4 shape parameters, $\mu, \kappa, 				\alpha, \beta$ and has the following structure:
  		\begin{align*}
  			p(c, \lambda) &= f(c, \lambda \mid \mu, \kappa, \alpha, \beta) \\
  			&= \mathcal{N}(c \mid \mu, 1/(\kappa \lambda)) \cdot \Gamma(\lambda 				\mid \alpha, \beta).
  		\end{align*}
  	\end{frame}
  	
  	\begin{frame}{Likelihood}
  		We recall that the measurements are defined as $\overline{y} = c + h +
  		\epsilon$. We assume independent noise for different points in time $t_1, 
  		\ldots, t_N, \ N \in \mathbb{N}$. Therefore the likelihood is
  		\begin{align*}
  			 p(D \mid \theta, c, \lambda) = \biggl(\frac{\lambda}{2\pi}\biggr) 
  			 ^{N/2} \cdot \exp\left( - \frac{\lambda}{2} \sum_{k = 1}^N 
  			 (\overline{y}_k - (c + h_k))^2 \right)
  		\end{align*}
  	\end{frame}
  	
	\begin{frame}{Marginal Likelihood}
  		The integral which we have to solve is defined as
  		\begin{align}
    		p(D \mid \theta) &= \int_{\R \times \R_+} p(D \mid \theta,c,\lambda) 				p(c, \lambda) \, d(c, \lambda)
		\end{align}
  	\end{frame}  	
  	
  	\begin{frame}{Marginal Likelihood}
  		The integral which we have to solve is defined as
  		\begin{align}
    		p(D \mid \theta) =& \int_{\R \times \R_+} p(D \mid \theta,c,\lambda) 				p(c, \lambda) \, d(c, \lambda) \\
    		=& \int_{\R_+} \int_\R \biggl(\frac{\lambda}{2\pi}\biggr)^{N/2} \exp				\left( - \frac{\lambda}{2} \sum_{k = 1}^N (\overline{y}_k - (c +
    		h_k))^2 \right) \\
    		&\cdot \frac{\beta^\alpha \sqrt{\kappa}}{\Gamma(\alpha)\sqrt{2\pi}} 				\lambda^{\alpha-1/2} \exp\left(- \frac{\lambda}{2} \bigl(\kappa (c - 
    		\mu)^2 + 2\beta\bigr) \right) \, dc \, d\lambda
		\end{align}
  	\end{frame}
  	
  	\begin{frame}{Marginal Likelihood}
  		With an exponential integration formula
  		\[
  			\int_\R \exp(-a \cdot c^2 + b \cdot c - d) \, dc = \sqrt{\frac{\pi}					{a}} \cdot \exp \biggl( \frac{b^2}{4a} - d \biggr)
  		\]
  	\end{frame}  	
  	
  	\begin{frame}{Marginal Likelihood}
  		With an exponential integration formula
  		\[
  			\int_\R \exp(-a \cdot c^2 + b \cdot c - d) \, dc = \sqrt{\frac{\pi}					{a}} \cdot \exp \biggl( \frac{b^2}{4a} - d \biggr)
  		\]
  		the substitution of $\varphi (\lambda) = C \cdot \lambda$ and the 
  		definition of the gamma-function we conclude with the following form:
  		\[
  			\frac{(\beta / C)^\alpha}{\Gamma(\alpha) (2\pi C)^{\frac{N}{2}}} \cdot 			\sqrt{\frac{\kappa}{N + \kappa}} \cdot \Gamma \biggl(\frac{N}{2} + 					\alpha \biggr).
  		\]
   	\end{frame}
   	
   	\begin{frame}{Distribution of $c$ and $\lambda$}
   		To sample $c$ and $\lambda$ in a second step we need to derive their 
   		distribution from the integrand of the marginalized likelihood. We have
   		\[
   			\lambda \propto \operatorname{Gamma}(\alpha' = \alpha + N/2, \beta' 
   			= C)
   		\]
		and
		\[
			c \propto \mathcal{N} \left(\mu' = \frac{\left(\sum_{k = 1}^N
			\overline{y_k}-h_k \right) + \kappa \mu }{N + \kappa}, \hat{\lambda}
			= \lambda (N + \kappa) \right)
		\]
   	\end{frame}
	
	\subsection{Conversion Reaction model}	
	
	\begin{frame}{Conversion Reaction model}
		For $k_1, k_2 \in \R$ we consider the following ordinary differential
		equation (ODE):
		\[
			\frac{d X_1(t)}{dt} = k_2 X_2 - k_1 X_1 \quad \quad 
			\frac{d X_2(t)}{dt} = k_2 X_1 - k_1 X_2.
		\]
		We want to observe $X_2$.
	\end{frame}
	
	\begin{frame}{Conversion Reaction model}
		For both approaches we sampled 50 independent runs with 10.000 steps each 
		with the Adaptive Metropolis sampler from pyPESTO.
	\end{frame}
	
	\subsection{mRNA-transfection model}




\section{Laplacian noise}

	\begin{frame}{Laplacian likelihood}
		Now we make the assumption that 
		\[
			\epsilon_k \sim Laplace(0, \sigma), \ \sigma \in (0, \infty)
		\]
		i.e. it has a Laplace distribution. The new likelihood has the following 
		form:
		\begin{align*}
    		p(D \mid \theta, c, \sigma) &= \prod_{k = 1}^N \operatorname{Laplace}
    		\ (\overline{y}_k	\mid c + h_k, \sigma) \\
    		&= \prod_{k = 1}^N \frac{1}{2\sigma} \cdot \exp \left\{- 							\frac{\lvert \overline{y}_k - c- h_k \lvert}{\sigma} \right\}
		\end{align*}
	\end{frame}
	
	\begin{frame}{Marginalisation Integral}
		The integral we receive is

		\begin{align}
    		&\iint p(D \mid \theta, c, \sigma) p(c) p(\sigma) \dx c \dx \sigma \\
    		&= \int_0^{\infty} \int_{-\infty}^{\infty} \prod_{k = 1}^N \frac{1}					{2\sigma} \cdot \exp \left\{- \frac{\lvert c - (\overline{y}_k -h_k) \lvert}					{\sigma} \right\} p(c) p(\sigma) \dx c \dx \sigma
		\end{align}
	\end{frame}
	
	\begin{frame}{Calculation}
		For calculation-reasons we renumber $\overline{y}_k$ and $h_k$ so that $y_k - h_k$ 			are ordered from smallest to biggest, i.e. $\overline{y}_1 - h_1$ is the smallest 			number, $\overline{y}_N - h_N$ the biggest.
		Then we choose $b_0 = -\infty, b_i = \overline{y}- h_i (i = 1, \ldots, N), b_{N+1} = 		\infty$. Now we can split up the integral in the following parts:
		\begin{align}
    		\int_0^{\infty} \sum_{i = 0}^N \int_{b_i}^{b_{i+1}} \frac{1}{2\sigma} 				\exp \left\{- \frac{\sum_{k = 1}^N \lvert c - (\overline{y}_k - h_k)\lvert} 					{\sigma} \right\} p(c) p(\sigma) \dx c \dx \sigma
		\end{align}
	\end{frame}
	
	\begin{frame}{Calculation}
		To remove the absolute value, we introduce the index $R_{k, i}$ which is 			defined like this:
		\[
			r_{k, i} =
			\begin{cases}
    			1 & \text{if} \, k \leq i \\
    			-1 & \text{else}
			\end{cases}
		\]
	\end{frame}
	
	\begin{frame}{Calculation}
		\begin{align}
    		&\int_0^{\infty} \frac{1}{2\sigma} \sum_{i = 0}^N p(\sigma) \int_{b_i}				^{b_{i+1}} \exp \underbrace{\left\{-\frac{\sum_{k = 1}^N r_{k, i} (c - 			(\overline{y}_k - h_k))}{\sigma} \right\}}_{= (*)} \dx c \dx\sigma \\
    		&\text{with} \, (*) = \frac{-c (i - (N -i)) + \sum_{k = 1}^i \overline{y}_k - 				h_k - \sum_{i + 1}^N \overline{y}_k - h_K}{\sigma} \\
    		&= \int_0^\infty \frac{1}{2\sigma} p(\sigma) \sum_{i = 0}^N \exp \left				\{\frac{\sum_{k = 1}^i \overline{y}_k - h_k - \sum_{i + 1}^N \overline{y}_k - h_K}{\sigma}				\right\} \\ \
    		&\cdot \int_{b_i}^{b_{i + 1}} e^{-c(2i -N)} \dx c \dx \sigma
		\end{align}
	\end{frame}
	
	\begin{frame}
		\begin{align}
    		& \int_0^\infty \frac{1}{2\sigma} \sum_{i = 1}^{N - 1} \exp \left
    		\{\frac{\sum_{k = 1}^i \overline{y}_k - h_k - \sum_{i + 1}^N
    		\overline{Y}_k - h_k}{\sigma} \right\} \\
    		& \cdot \frac{1}{N - 2i} \cdot \Bigl(e^{-b_{i + 1}(2i - N)} - e^{-b_i(2i - N)} \Bigr) \dx 					\sigma  \\
    		& + \int_0^\infty \frac{1}{2\sigma} \exp\left\{\frac{-\sum_{k = 1}^N 				\overline{y}_k - h_k}{\sigma} \right\} \underbrace{\int_{-\infty}^{b_1} e^{Nc} 				\dx c}_{\frac{1}{N}e^{N b_1}} \dx \sigma \\
    		&+ \int_0^\infty \frac{1}{2\sigma} \exp\left\{\frac{\sum_{k = 1}^N 					\overline{y}_k - h_k}{\sigma} \right\} \underbrace{\int_{b_N}^{\infty} e^{-Nc} 				\dx c}_{\frac{1}{N}e^{-N b_N}} \dx \sigma
		\end{align}
	\end{frame}
	
\end{document}





















