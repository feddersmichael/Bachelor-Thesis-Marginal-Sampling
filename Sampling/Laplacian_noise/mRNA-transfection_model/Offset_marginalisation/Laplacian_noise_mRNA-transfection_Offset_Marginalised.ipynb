{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'corrupted_data/SS_conversion_reaction_original.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-f533c614b39d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;31m# import to petab\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mdatatype\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"original\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m     petab_problem = petab.Problem.from_yaml(\n\u001B[0m\u001B[0;32m     23\u001B[0m     \"corrupted_data/SS_conversion_reaction_original.yaml\")\n\u001B[0;32m     24\u001B[0m \u001B[1;32melif\u001B[0m \u001B[0mdatatype\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"switch\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\petab\\problem.py\u001B[0m in \u001B[0;36mfrom_yaml\u001B[1;34m(yaml_config)\u001B[0m\n\u001B[0;32m    150\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0myaml_config\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m             \u001B[0mpath_prefix\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdirname\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0myaml_config\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 152\u001B[1;33m             \u001B[0myaml_config\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0myaml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_yaml\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0myaml_config\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    153\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m             \u001B[0mpath_prefix\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\petab\\yaml.py\u001B[0m in \u001B[0;36mload_yaml\u001B[1;34m(yaml_config)\u001B[0m\n\u001B[0;32m    127\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    128\u001B[0m         \u001B[1;31m# a filename\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 129\u001B[1;33m         \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'r'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    130\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0myaml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msafe_load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    131\u001B[0m     \u001B[1;31m# a stream\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'corrupted_data/SS_conversion_reaction_original.yaml'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import pypesto\n",
    "import pypesto.petab\n",
    "import pypesto.optimize as optimize\n",
    "import pypesto.sample as sample\n",
    "import pypesto.visualize as visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import petab\n",
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "from scipy.special import gamma\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "datatype = \"original\"\n",
    "\n",
    "# import to petab\n",
    "if datatype == \"original\":\n",
    "    petab_problem = petab.Problem.from_yaml(\n",
    "    \"corrupted_data/SS_conversion_reaction_original.yaml\")\n",
    "elif datatype == \"switch\":\n",
    "    petab_problem = petab.Problem.from_yaml(\n",
    "    \"corrupted_data/SS_conversion_reaction_switch.yaml\")\n",
    "elif datatype == \"loss\":\n",
    "    petab_problem = petab.Problem.from_yaml(\n",
    "    \"corrupted_data/SS_conversion_reaction_loss.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def analytical_b(t, a0, b0, k1, k2):\n",
    "    return (k2 - k2 * np.exp(-(k2 + k1) * t)) / (k2 + k1)\n",
    "\n",
    "def simulate_model(x, tvec):\n",
    "    # assign parameters\n",
    "    k1, k2, _ = x\n",
    "    # define initial conditions\n",
    "    a0 = 1\n",
    "    b0 = 0\n",
    "    # simulate model\n",
    "    simulation = [analytical_b(t, a0, b0, k1, k2)\n",
    "                   for t in tvec]\n",
    "    return simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the model, we need to define the objective function. This time we will do it via an external function that will be used then by pyPESTO instead of using the built-in ones.\n",
    "\n",
    "For numerical reasons we will implement the log likelihood and log prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\log(p(D \\mid \\theta, \\sigma)) =& \\log(\\lambda) - N \\cdot ( log(\\sigma) + \\log(2)) + \\log \\left( \\sum_{i = r}^N k_i \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Hereby $k_i$ is defined differently depending on the value of $\\frac{N - 2i}{\\sigma} - \\lambda$. We have\n",
    " \\begin{align*}\n",
    "     k_r \\equiv\n",
    "     \\begin{cases}\n",
    "         \\displaystyle \\frac{\\sigma}{N -2r - \\sigma\\lambda} \\cdot \\left(\\exp \\left( \\frac{b_{r + 1}(N - 2r) + l_r}{\\sigma} - b_{r + 1}\\lambda \\right) - \\exp \\left( \\frac{l_r}{\\sigma} \\right) \\right), &\\text{if} \\; \\frac{N - 2r}{\\sigma} - \\lambda \\neq 0 \\\\\n",
    "         \\displaystyle b_{r + 1} \\exp \\left( \\frac{l_r}{\\sigma} \\right), &\\text{if} \\; \\frac{N - 2r}{\\sigma} - \\lambda = 0\n",
    "     \\end{cases}\n",
    " \\end{align*}\n",
    " and for $i = r + 1, \\ldots, N -1$\n",
    " \\begin{align*}\n",
    "     k_i \\equiv\n",
    "     \\begin{cases}\n",
    "         \\frac{\\sigma}{N -2i - \\sigma\\lambda} \\cdot \\left(\\exp \\left( \\frac{b_{i + 1}(N - 2i) + l_i}{\\sigma} - b_{i + 1}\\lambda \\right) - \\exp\\left( \\frac{b_{i}(N - 2i) + l_i}{\\sigma} - b_{i}\\lambda \\right) \\right), &\\text{if} \\; \\frac{N - 2i}{\\sigma} - \\lambda \\neq 0 \\\\\n",
    "         \\displaystyle \\left(b_{i + 1} - b_i \\right) \\exp \\left( \\frac{l_i}{\\sigma} \\right), &\\text{if} \\; \\frac{N - 2i}{\\sigma} - \\lambda = 0\n",
    "     \\end{cases}\n",
    " \\end{align*}\n",
    " and because $-N / \\sigma - \\lambda < 0$ we have always\n",
    " \\begin{align*}\n",
    "     k_N \\equiv \\frac{1}{N + \\sigma\\lambda} \\cdot \\exp \\biggl(\\frac{-b_N N + l_N}{\\sigma} - b_N \\lambda \\biggr).\n",
    " \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def negative_log_marginalised_likelihood(x):\n",
    "    shape = x[2]\n",
    "    \n",
    "    data = np.asarray(petab_problem.measurement_df.measurement)\n",
    "    tvec = np.asarray(petab_problem.measurement_df.time)\n",
    "    N = len(tvec)\n",
    "    \n",
    "    # simulate model\n",
    "    _simulation = simulate_model(np.exp(x), tvec)\n",
    "    simulation = np.asarray(_simulation)\n",
    "    \n",
    "    # evaluate standard log likelihood\n",
    "    #We sort the difference of data and simulation in increasing order\n",
    "    res = data - simulation\n",
    "    b_vector = np.sort(res)\n",
    "    bounds = np.append(np.append(-np.inf, b_vector), np.inf)\n",
    "    #r is the greatest index such that bounds[r] < 0\n",
    "    #especially r in the code has the same value as r in the derivation\n",
    "    r = np.argmax(bounds >= 0)-1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #special case i = r\n",
    "    check = (N - 2*r)/shape - lamda\n",
    "    #index choice because we use b_vector not bounds\n",
    "    l_value = np.sum(b_vector[:r]) - np.sum(b_vector[r:])\n",
    "    tmp = l_value/shape\n",
    "    if check != 0:\n",
    "        aux1 = 1 / check\n",
    "        aux2 = (np.exp(tmp + bounds[r+1]*check)-np.exp(tmp))\n",
    "    else:\n",
    "        aux1 = bounds[r + 1]\n",
    "        aux2 = np.exp(tmp)\n",
    "    \n",
    "    marginal_posterior = aux1 * aux2\n",
    "    \n",
    "    #general case i = r+1, ..., N-1\n",
    "    for n in range(r + 1, N):\n",
    "        #It is sufficient to add the change\n",
    "        l_value += 2 * bounds[n]\n",
    "        tmp = l_value / shape\n",
    "        check = (N - 2*n)/shape - lamda\n",
    "        \n",
    "        if check != 0:\n",
    "            aux1 = 1 / check\n",
    "            aux2 = np.exp(tmp + bounds[n+1]*check) - np.exp(tmp + bounds[n]*check)\n",
    "        else:\n",
    "            aux1 = bounds[n + 1]\n",
    "            aux2 = np.exp(tmp)\n",
    "        \n",
    "        \n",
    "        #here we add up the k_i values\n",
    "        marginal_posterior += aux1*aux2\n",
    "        \n",
    "        \n",
    "    #special case i = N\n",
    "    l_value += 2 * bounds[N]\n",
    "    tmp = l_value / shape\n",
    "    aux1 = 1/(N + shape*lamda)\n",
    "    aux2 = np.exp(tmp - bounds[N]*(N/shape + lamda))\n",
    "    \n",
    "    marginal_posterior += aux1 * aux2\n",
    "        \n",
    "        \n",
    "        \n",
    "    log_marginal_posterior = np.log(marginal_posterior)\n",
    "    log_marginal_posterior += -N*(np.log(2) + np.log(shape)) + np.log(lamda)\n",
    "    \n",
    "    return -log_marginal_posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the objective function defined, we need to create a pyPESTO problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def marginal_sampling():\n",
    "    \"\"\"Creates a pyPESTO problem.\"\"\"\n",
    "    objective = pypesto.Objective(fun=negative_log_marginalised_likelihood)\n",
    "    problem = pypesto.Problem(objective=objective,  # objective function\n",
    "                              lb=[-5, -5, 0],  # lower bounds\n",
    "                              ub=[5, 5, np.inf],  # upper bounds\n",
    "                              x_names=['k1', 'k2', 'shape'],  # parameter names\n",
    "                              x_scales=['log', 'log', 'lin'])  # parameter scale\n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Prior dependent paramters\n",
    "lamda = 0.01\n",
    "\n",
    "# create the estimation problem\n",
    "problem = marginal_sampling()\n",
    "\n",
    "# MCMC chain length\n",
    "n_samples= 100000\n",
    "\n",
    "# call the sampler of choice\n",
    "sampler = sample.AdaptiveMetropolisSampler()\n",
    "\n",
    "x0=np.array([-1.2741, -0.6160, 0.3684])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform the actual sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define number of runs\n",
    "runs = 1\n",
    "\n",
    "save_results = False # for testing just set to False\n",
    "\n",
    "# Loop over n runs\n",
    "for n in range(runs):\n",
    "    # set initial random seed\n",
    "    np.random.seed(n)\n",
    "    # perform MCMC sampling\n",
    "    result = sample.sample(problem, n_samples=n_samples, sampler=sampler,\n",
    "                           x0=x0)\n",
    "    # calculate effective sample size\n",
    "    sample.effective_sample_size(result=result)\n",
    "\n",
    "    # save the results as a pickle object\n",
    "    if save_results:\n",
    "        results = [result.sample_result]\n",
    "        with open('Results/Offset_marginalized/' + str(n) + '.pickle','wb') as result_file:\n",
    "            pickle.dump(results, result_file, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some built-in visualization functions that one can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = visualize.sampling_fval_trace(result,full_trace=True)\n",
    "# Visualize the parameter trace\n",
    "ax = visualize.sampling.sampling_parameters_trace(result, use_problem_bounds=False,\n",
    "                                                  full_trace=True, size=(12,5))\n",
    "# Visualize the one-dimensional marginals\n",
    "ax = visualize.sampling_1d_marginals(result, size=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to sample the offset $c$ from our generated data. It is distributed as a piecewise exponential distribution with this density function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\mathbf{1}_{[0, b_{r + 1})}(c) \\exp\\biggl( c \\cdot \\biggl( \\frac{N - 2r}{\\sigma} - \\lambda \\biggr) + \\frac{l_r}{\\sigma} \\biggr) + \\sum_{i = r + 1}^N \\mathbf{1}_{[b_i, b_{i + 1})}(c) \\exp \\biggl( c \\cdot \\biggl( \\frac{N - 2i}{\\sigma} - \\lambda \\biggr) + \\frac{l_i}{\\sigma} \\biggr)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample this distribution we also need the mass of the corresponding integral over every section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "I_r &=\n",
    "\\begin{cases}\n",
    "         \\displaystyle \\frac{\\sigma}{N -2r - \\sigma\\lambda} \\cdot \\left(\\exp \\left( \\frac{b_{r + 1}(N - 2r) + l_r}{\\sigma} - b_{r + 1}\\lambda \\right) - \\exp \\left( \\frac{l_r}{\\sigma} \\right) \\right), &\\text{if} \\; \\frac{N - 2r}{\\sigma} - \\lambda \\neq 0 \\\\\n",
    "         \\displaystyle b_{r + 1} \\exp \\left( \\frac{l_r}{\\sigma} \\right), &\\text{if} \\; \\frac{N - 2r}{\\sigma} - \\lambda = 0\n",
    "\\end{cases}\\\\\n",
    "I_{i = r+1, \\ldots, N-1} &= \\begin{cases}\n",
    "         \\frac{\\sigma}{N -2i - \\sigma\\lambda} \\cdot \\left(\\exp \\left( \\frac{b_{i + 1}(N - 2i) + l_i}{\\sigma} - b_{i + 1}\\lambda \\right) - \\exp\\left( \\frac{b_{i}(N - 2i) + l_i}{\\sigma} - b_{i}\\lambda \\right) \\right), &\\text{if} \\; \\frac{N - 2i}{\\sigma} - \\lambda \\neq 0 \\\\\n",
    "         \\displaystyle \\left(b_{i + 1} - b_i \\right) \\exp \\left( \\frac{l_i}{\\sigma} \\right), &\\text{if} \\; \\frac{N - 2i}{\\sigma} - \\lambda = 0\n",
    "     \\end{cases} \\\\\n",
    "I_N &= \\frac{1}{N + \\lambda\\sigma} \\cdot \\exp \\biggl(\\frac{-b_N N + l_N}{\\sigma} - b_N \\lambda \\biggr)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a piecewise defined probability distribution we use the weights of the corresponding integrals to choose with a uniformly distributed random variable $s$ in which part we are sampling. Afterwards we generate a random variable with the fitting shpae on that onterval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sample uniformly distributed $s_1$ and determine smallest $i \\in \\{r, \\ldots, N\\}$ such that $s_1 \\leq \\frac{\\sum_{k = r}^i I_k}{\\sum_{l = r}^N I_l}$\n",
    "\n",
    "2. We now want to sample the according random variable with support inside of $[b_i, b_{i + 1}]$ (with $b_r \\equiv 0$).\n",
    "\n",
    "3. We start by sampling a uniformly distributed random variable $s$ on $[0, 1]$. If $\\frac{N - 2i}/\\sigma - \\lambda = 0$ we can just scale $s$ on $[b_i, b_{i + 1}]$. Else we can use the transformation \n",
    "\\begin{align*}\n",
    "        f(s) \\equiv \\frac{\\log \\left( \\exp \\left( \\left(\\frac{N - 2i}{\\sigma} - \\lambda \\right) \\cdot b_i \\right) + s \\cdot \\left( \\exp \\left( \\left(\\frac{N - 2i}{\\sigma} - \\lambda \\right) \\cdot b_{i + 1} \\right) - \\exp \\left( \\left(\\frac{N - 2i}{\\sigma} - \\lambda \\right) \\cdot b_i \\right) \\right) \\right)}{\\frac{N - 2i}{\\sigma} - \\lambda}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset(data, simulation, shape, lamda):\n",
    "    res = data - simulation\n",
    "    b_vector = np.sort(res)\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    bounds = np.append(np.append(-np.inf, b_vector), np.inf)\n",
    "    r = np.argmax(bounds >= 0)-1\n",
    "    \n",
    "    probability_mass = np.zeros(N + 1 - r)\n",
    "    \n",
    "    #special case i = r\n",
    "    check = (N - 2*r)/shape - lamda\n",
    "    #index choice because we use b_vector not bounds\n",
    "    l_value = np.sum(b_vector[:r]) - np.sum(b_vector[r:])\n",
    "    tmp = l_value/shape\n",
    "    if check != 0:\n",
    "        aux1 = 1 / check\n",
    "        aux2 = (np.exp(tmp + bounds[r+1]*check)-np.exp(tmp))\n",
    "    else:\n",
    "        aux1 = bounds[r + 1]\n",
    "        aux2 = np.exp(tmp)\n",
    "    \n",
    "    probability_mass[0] = aux1 * aux2\n",
    "    \n",
    "    #general case i = r+1, ..., N-1\n",
    "    for n in range(r + 1, N):\n",
    "        #It is sufficient to add the change\n",
    "        l_value += 2 * bounds[n]\n",
    "        tmp = l_value / shape\n",
    "        check = (N - 2*n)/shape - lamda\n",
    "        \n",
    "        if check != 0:\n",
    "            aux1 = 1 / check\n",
    "            aux2 = np.exp(tmp + bounds[n+1]*check) - np.exp(tmp + bounds[n]*check)\n",
    "        else:\n",
    "            aux1 = bounds[n + 1]\n",
    "            aux2 = np.exp(tmp)\n",
    "        \n",
    "        \n",
    "        #here we add up the k_i values\n",
    "        probability_mass[n - r] = probability_mass[n - r - 1] + aux1*aux2\n",
    "        \n",
    "        \n",
    "    #special case i = N\n",
    "    l_value += 2 * bounds[N]\n",
    "    tmp = l_value / shape\n",
    "    aux1 = 1/(N + shape*lamda)\n",
    "    aux2 = np.exp(tmp - bounds[N]*(N/shape + lamda))\n",
    "    \n",
    "    normalisation_constant = probability_mass[N - r -1] + aux1*aux2\n",
    "    \n",
    "    probability_mass = probability_mass / normalisation_constant\n",
    "    probability_mass[N-r] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    s = Generator.uniform(size = 2)\n",
    "    i = np.argmax(probability_mass >= s[0])\n",
    "\n",
    "    factor = (N - 2*(i + r))/shape - lamda\n",
    "    if factor == 0:\n",
    "        if i == 0:\n",
    "            offset = s[1] * bounds[r + 1]\n",
    "        else:\n",
    "            offset = s[1] * (bounds[i + 1 + r] - bounds[i + r])\n",
    "    else:\n",
    "        if i == 0:\n",
    "            lb = 0\n",
    "        else:\n",
    "            lb = bounds[i + r]\n",
    "\n",
    "        if i == N - r:\n",
    "            offset = np.log(1 - s[1])/ factor + lb\n",
    "        else:\n",
    "            ub = bounds[i + 1 + r]\n",
    "            compensate = factor * ub\n",
    "            offset = (compensate + np.log(np.exp(factor*lb - compensate) + s[1] *(np.exp(factor*ub - compensate) - np.exp(factor*lb - compensate)))) / factor\n",
    "            \n",
    "    return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tvec = np.asarray(petab_problem.measurement_df.time)\n",
    "data = np.asarray(petab_problem.measurement_df.measurement)\n",
    "\n",
    "Generator = np.random.default_rng()\n",
    "\n",
    "offset_samples = np.zeros([np.shape(\n",
    "    result.sample_result.trace_x[0, result.sample_result.burn_in:, 0])[0], 1])\n",
    "\n",
    "for index, parameter_sample in enumerate(result.sample_result.trace_x[0, result.sample_result.burn_in:, :]):\n",
    "    shape = parameter_sample[-1]\n",
    "    _simulation = simulate_model(np.exp(parameter_sample), tvec)\n",
    "    simulation = np.asarray(_simulation)\n",
    "    \n",
    "    offset_samples[index, :] = get_offset(data, simulation, shape, lamda)\n",
    "    \n",
    "if save_results:\n",
    "    results = [result.sample_result, offset_samples]\n",
    "    with open('Results/Offset_marginalized/' + str(n) + '.pickle','wb') as result_file:\n",
    "        pickle.dump(results, result_file, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = sns.kdeplot(offset_samples[:,0], shade=True, color='C0')\n",
    "plt.xlabel('offset')\n",
    "plt.ylabel('kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_MAP = True\n",
    "\n",
    "if plot_MAP:\n",
    "    MAP_index = np.argmax(-result.sample_result.trace_neglogpost[0,result.sample_result.burn_in:])\n",
    "    MAP = result.sample_result.trace_x[0,result.sample_result.burn_in+MAP_index,:]\n",
    "    print(MAP)\n",
    "    \n",
    "    # experimental data\n",
    "    data = np.asarray(petab_problem.measurement_df.measurement)\n",
    "    # time vector\n",
    "    tvec = np.asarray(petab_problem.measurement_df.time)\n",
    "\n",
    "    tvec_for_plotting = np.linspace(tvec[0],tvec[-1],100)\n",
    "    \n",
    "    scale_MAP = MAP[-1]\n",
    "\n",
    "    # simulate model\n",
    "    _simulation = simulate_model(np.exp(MAP), tvec)\n",
    "    simulation = np.asarray(_simulation)\n",
    "    \n",
    "    offset_MAP = get_offset(data, simulation, scale_MAP, lamda)\n",
    "    print(offset_MAP)\n",
    "    \n",
    "    _simulation = simulate_model(np.exp(MAP), tvec_for_plotting)\n",
    "    simulation_for_plotting = np.asarray(offset_MAP + _simulation)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(tvec,data,'or',label='Corrupted data')\n",
    "    plt.plot(tvec_for_plotting,simulation_for_plotting,'k',label='MAP simulation')\n",
    "    plt.xlabel('Time [a.u.]')\n",
    "    plt.ylabel('Signal [a.u.]')\n",
    "    #plt.ylim([0,2])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pyPESTO)",
   "language": "python",
   "name": "pycharm-5c126572"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}