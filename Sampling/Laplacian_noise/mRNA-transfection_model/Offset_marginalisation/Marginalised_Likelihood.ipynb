{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pypesto\n",
    "import pypesto.petab\n",
    "import pypesto.sample as sample\n",
    "import pypesto.visualize as visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"loss\"\n",
    "\n",
    "# Load experimental data\n",
    "if datatype == \"original\":\n",
    "    df=pd.read_csv('data/data.csv', sep='\\t')\n",
    "elif datatype == \"switch\":\n",
    "    df=pd.read_csv('data/data_switch.csv', sep='\\t')\n",
    "else:\n",
    "    df=pd.read_csv('data/data_loss.csv', sep='\\t')\n",
    "    \n",
    "add_offset_to_data = True\n",
    "offset_value = 0.2\n",
    "\n",
    "if add_offset_to_data:\n",
    "    df.Measurement=df.Measurement+offset_value\n",
    "    \n",
    "data = np.asarray(df.Measurement)\n",
    "\n",
    "tvec = np.asarray(df.Time)\n",
    "    \n",
    "N = len(tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def analytical_x2(t, t0, kTL_m0, xi, delta):\n",
    "    X = [np.exp(-delta*(t-t0)) * (t>t0),\n",
    "         kTL_m0 * (np.exp(-xi*(t-t0)) - np.exp(-delta*(t-t0))) / (delta-xi) * (t>t0)]\n",
    "    return X[1]\n",
    "\n",
    "def simulate_model(x, tvec):\n",
    "    # assign parameters\n",
    "    t0, kTL_m0, xi, delta, _ = x\n",
    "    # simulate model\n",
    "    simulation = np.asarray([analytical_x2(t, t0, kTL_m0, xi, delta)\n",
    "                             for t in tvec])\n",
    "    return simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the model, we need to define the objective function. This time we will do it via an external function that will be used then by pyPESTO instead of using the built-in ones.\n",
    "\n",
    "For numerical reasons we will implement the log likelihood and log prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\log(p(D \\mid \\theta, \\sigma)) =& \\log(\\lambda) - N \\cdot ( log(\\sigma) + \\log(2)) + \\log \\left( \\sum_{i = r}^N k_i \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Hereby $k_i$ is defined differently depending on the value of $\\frac{N - 2i}{\\sigma} - \\lambda$. We have\n",
    " \\begin{align*}\n",
    "     k_r \\equiv\n",
    "     \\begin{cases}\n",
    "         \\displaystyle \\frac{\\sigma}{N -2r - \\sigma\\lambda} \\cdot \\left(\\exp \\left( \\frac{b_{r + 1}(N - 2r) + l_r}{\\sigma} - b_{r + 1}\\lambda \\right) - \\exp \\left( \\frac{l_r}{\\sigma} \\right) \\right), &\\text{if} \\; \\frac{N - 2r}{\\sigma} - \\lambda \\neq 0 \\\\\n",
    "         \\displaystyle b_{r + 1} \\exp \\left( \\frac{l_r}{\\sigma} \\right), &\\text{if} \\; \\frac{N - 2r}{\\sigma} - \\lambda = 0\n",
    "     \\end{cases}\n",
    " \\end{align*}\n",
    " and for $i = r + 1, \\ldots, N -1$\n",
    " \\begin{align*}\n",
    "     k_i \\equiv\n",
    "     \\begin{cases}\n",
    "         \\frac{\\sigma}{N -2i - \\sigma\\lambda} \\cdot \\left(\\exp \\left( \\frac{b_{i + 1}(N - 2i) + l_i}{\\sigma} - b_{i + 1}\\lambda \\right) - \\exp\\left( \\frac{b_{i}(N - 2i) + l_i}{\\sigma} - b_{i}\\lambda \\right) \\right), &\\text{if} \\; \\frac{N - 2i}{\\sigma} - \\lambda \\neq 0 \\\\\n",
    "         \\displaystyle \\left(b_{i + 1} - b_i \\right) \\exp \\left( \\frac{l_i}{\\sigma} \\right), &\\text{if} \\; \\frac{N - 2i}{\\sigma} - \\lambda = 0\n",
    "     \\end{cases}\n",
    " \\end{align*}\n",
    " and because $-N / \\sigma - \\lambda < 0$ we have always\n",
    " \\begin{align*}\n",
    "     k_N \\equiv \\frac{1}{N + \\sigma\\lambda} \\cdot \\exp \\biggl(\\frac{-b_N N + l_N}{\\sigma} - b_N \\lambda \\biggr).\n",
    " \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def negative_log_marginalised_likelihood(x):\n",
    "    shape = x[4]\n",
    "    \n",
    "    # simulate model\n",
    "    _simulation = simulate_model(np.power(10, x), tvec)\n",
    "    simulation = np.asarray(_simulation)\n",
    "    \n",
    "    # evaluate standard log likelihood\n",
    "    #We sort the difference of data and simulation in increasing order\n",
    "    res = data - simulation\n",
    "    b_vector = np.sort(res)\n",
    "    bounds = np.append(np.append(-np.inf, b_vector), np.inf)\n",
    "    #r is the greatest index such that bounds[r] < 0\n",
    "    #especially r in the code has the same value as r in the derivation\n",
    "    r = np.argmax(bounds >= 0)-1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #special case i = r\n",
    "    check = (N - 2*r)/shape - lamda\n",
    "    #index choice because we use b_vector not bounds\n",
    "    l_value = np.sum(b_vector[:r]) - np.sum(b_vector[r:])\n",
    "    tmp = l_value/shape\n",
    "    if check != 0:\n",
    "        aux1 = 1 / check\n",
    "        aux2 = (np.exp(tmp + bounds[r+1]*check)-np.exp(tmp))\n",
    "    else:\n",
    "        aux1 = bounds[r + 1]\n",
    "        aux2 = np.exp(tmp)\n",
    "    \n",
    "    marginal_posterior = aux1 * aux2\n",
    "    \n",
    "    #general case i = r+1, ..., N-1\n",
    "    for n in range(r + 1, N):\n",
    "        #It is sufficient to add the change\n",
    "        l_value += 2 * bounds[n]\n",
    "        tmp = l_value / shape\n",
    "        check = (N - 2*n)/shape - lamda\n",
    "        \n",
    "        if check != 0:\n",
    "            aux1 = 1 / check\n",
    "            aux2 = np.exp(tmp + bounds[n+1]*check) - np.exp(tmp + bounds[n]*check)\n",
    "        else:\n",
    "            aux1 = bounds[n + 1]\n",
    "            aux2 = np.exp(tmp)\n",
    "        \n",
    "        \n",
    "        #here we add up the k_i values\n",
    "        marginal_posterior += aux1*aux2\n",
    "        \n",
    "        \n",
    "    #special case i = N\n",
    "    l_value += 2 * bounds[N]\n",
    "    tmp = l_value / shape\n",
    "    aux1 = 1/(N + shape*lamda)\n",
    "    aux2 = np.exp(tmp - bounds[N]*(N/shape + lamda))\n",
    "    \n",
    "    marginal_posterior += aux1 * aux2\n",
    "        \n",
    "        \n",
    "        \n",
    "    log_marginal_posterior = np.log(marginal_posterior)\n",
    "    log_marginal_posterior += -N*(np.log(2) + np.log(shape)) + np.log(lamda)\n",
    "    \n",
    "    return -log_marginal_posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the objective function defined, we need to create a pyPESTO problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def marginal_sampling():\n",
    "    \"\"\"Creates a pyPESTO problem.\"\"\"\n",
    "    objective = pypesto.Objective(fun=negative_log_marginalised_likelihood)\n",
    "    problem = pypesto.Problem(objective=objective,  # objective function\n",
    "                              lb=[-2, -5, -5, -5, np.exp(-5)],  # lower bounds\n",
    "                              ub=[np.log10(df.Time.max()), 5, 5, 5, np.exp(5)],  # upper bounds\n",
    "                              x_names=['t_0', 'k_{TL}*m_0', 'xi', 'delta', 'shape'],  # parameter names\n",
    "                              x_scales=['log10', 'log10', 'log10', 'log10', 'lin'])  # parameter scale\n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Prior dependent paramters\n",
    "lamda = 0.01\n",
    "\n",
    "# create the estimation problem\n",
    "problem = marginal_sampling()\n",
    "\n",
    "# MCMC chain length\n",
    "n_samples= 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [np.array([0.2998, 0.9949, -0.1074, -0.6910, 0.2]),\n",
    "      np.array([0.2998, 0.9949, -0.6910, -0.1074, 0.2]),\n",
    "      np.array([0.2998, 0.9949, -0.1074, -0.6910, 0.2]),\n",
    "      np.array([0.2998, 0.9949, -0.6910, -0.1074, 0.2]),\n",
    "      np.array([0.2998, 0.9949, -0.1074, -0.6910, 0.2]),\n",
    "      np.array([0.2998, 0.9949, -0.6910, -0.1074, 0.2]),\n",
    "      np.array([0.2998, 0.9949, -0.1074, -0.6910, 0.2]),\n",
    "      np.array([0.2998, 0.9949, -0.6910, -0.1074, 0.2]),\n",
    "      np.array([0.2998, 0.9949, -0.1074, -0.6910, 0.2]),\n",
    "      np.array([0.2998, 0.9949, -0.6910, -0.1074, 0.2])]\n",
    "\n",
    "cov0 = 1e-4\n",
    "\n",
    "# call the sampler of choice\n",
    "sampler = sample.AdaptiveParallelTemperingSampler(n_chains=10, internal_sampler=\n",
    "                                                  sample.AdaptiveMetropolisSampler(options={'cov0': cov0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform the actual sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 547073/1000000 [1:36:38<1:20:01, 94.34it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-4c1bd56b0e1a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[1;31m# perform MCMC sampling\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m     result = sample.sample(problem, n_samples=n_samples, sampler=sampler,\n\u001B[0m\u001B[0;32m     12\u001B[0m                            x0=x0)\n\u001B[0;32m     13\u001B[0m     \u001B[1;31m# calculate effective sample size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\sample\\sample.py\u001B[0m in \u001B[0;36msample\u001B[1;34m(problem, n_samples, sampler, x0, result)\u001B[0m\n\u001B[0;32m     66\u001B[0m     \u001B[1;31m# perform the sampling and track time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m     \u001B[0mt_start\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_time\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 68\u001B[1;33m     \u001B[0msampler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_samples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mn_samples\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     69\u001B[0m     \u001B[0mt_elapsed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_time\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mt_start\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m     \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Elapsed time: \"\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt_elapsed\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\sample\\parallel_tempering.py\u001B[0m in \u001B[0;36msample\u001B[1;34m(self, n_samples, beta)\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[1;31m# sample\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0msampler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbeta\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msamplers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbetas\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m                 \u001B[0msampler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_samples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbeta\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbeta\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;31m# swap samples\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\sample\\metropolis.py\u001B[0m in \u001B[0;36msample\u001B[1;34m(self, n_samples, beta)\u001B[0m\n\u001B[0;32m     55\u001B[0m             \u001B[0mi\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     56\u001B[0m             \u001B[1;31m# perform step\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 57\u001B[1;33m             x, lpost, lprior = self._perform_step(\n\u001B[0m\u001B[0;32m     58\u001B[0m                 x=x, lpost=lpost, lprior=lprior, beta=beta)\n\u001B[0;32m     59\u001B[0m             \u001B[1;31m# record step\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\sample\\metropolis.py\u001B[0m in \u001B[0;36m_perform_step\u001B[1;34m(self, x, lpost, lprior, beta)\u001B[0m\n\u001B[0;32m     82\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m             \u001B[1;31m# compute log posterior\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 84\u001B[1;33m             \u001B[0mlpost_new\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mneglogpost\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_new\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     85\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m         \u001B[1;31m# check posterior evaluation is successful\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\objective\\base.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, x, sensi_orders, mode, return_dict)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    144\u001B[0m         \u001B[1;31m# compute result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 145\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall_unprocessed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_full\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msensi_orders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    146\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    147\u001B[0m         \u001B[1;31m# post-process\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\objective\\function.py\u001B[0m in \u001B[0;36mcall_unprocessed\u001B[1;34m(self, x, sensi_orders, mode)\u001B[0m\n\u001B[0;32m    141\u001B[0m         \"\"\"\n\u001B[0;32m    142\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mMODE_FUN\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 143\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_mode_fun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msensi_orders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    144\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mmode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mMODE_RES\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    145\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_mode_res\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msensi_orders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\objective\\function.py\u001B[0m in \u001B[0;36m_call_mode_fun\u001B[1;34m(self, x, sensi_orders)\u001B[0m\n\u001B[0;32m    155\u001B[0m                 \u001B[0mfval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 157\u001B[1;33m                 \u001B[0mfval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    158\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[0mFVAL\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfval\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-4-2bd504ccfa20>\u001B[0m in \u001B[0;36mnegative_log_marginalised_likelihood\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[1;31m#We sort the difference of data and simulation in increasing order\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mres\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0msimulation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m     \u001B[0mb_vector\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msort\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mres\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m     \u001B[0mbounds\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb_vector\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[1;31m#r is the greatest index such that bounds[r] < 0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36msort\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001B[0m in \u001B[0;36msort\u001B[1;34m(a, axis, kind, order)\u001B[0m\n\u001B[0;32m    989\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    990\u001B[0m         \u001B[0ma\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0masanyarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0morder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"K\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 991\u001B[1;33m     \u001B[0ma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msort\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkind\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkind\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0morder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0morder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    992\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    993\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Define number of runs\n",
    "runs = 1\n",
    "\n",
    "save_results = False # for testing just set to False\n",
    "\n",
    "# Loop over n runs\n",
    "for n in range(runs):\n",
    "    # set initial random seed\n",
    "    np.random.seed(n)\n",
    "    # perform MCMC sampling\n",
    "    result = sample.sample(problem, n_samples=n_samples, sampler=sampler,\n",
    "                           x0=x0)\n",
    "    # calculate effective sample size\n",
    "    sample.effective_sample_size(result=result)\n",
    "\n",
    "    # save the results as a pickle object\n",
    "    if save_results:\n",
    "        results = [result.sample_result]\n",
    "        with open('Results/Offset_marginalized/' + str(n) + '.pickle','wb') as result_file:\n",
    "            pickle.dump(results, result_file, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some built-in visualization functions that one can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = visualize.sampling.sampling_fval_traces(result,full_trace=True)\n",
    "# Visualize the parameter trace\n",
    "ax = visualize.sampling.sampling_parameters_trace(result, use_problem_bounds=False,\n",
    "                                                  full_trace=True, size=(12,5))\n",
    "# Visualize the one-dimensional marginals\n",
    "ax = visualize.sampling_1d_marginals(result, size=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to sample the offset $c$ from our generated data. It is distributed as a piecewise exponential distribution with this density function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\mathbf{1}_{[0, b_{r + 1})}(c) \\exp\\biggl( c \\cdot \\biggl( \\frac{N - 2r}{\\sigma} - \\lambda \\biggr) + \\frac{l_r}{\\sigma} \\biggr) + \\sum_{i = r + 1}^N \\mathbf{1}_{[b_i, b_{i + 1})}(c) \\exp \\biggl( c \\cdot \\biggl( \\frac{N - 2i}{\\sigma} - \\lambda \\biggr) + \\frac{l_i}{\\sigma} \\biggr)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample this distribution we also need the mass of the corresponding integral over every section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "I_r &=\n",
    "\\begin{cases}\n",
    "         \\displaystyle \\frac{\\sigma}{N -2r - \\sigma\\lambda} \\cdot \\left(\\exp \\left( \\frac{b_{r + 1}(N - 2r) + l_r}{\\sigma} - b_{r + 1}\\lambda \\right) - \\exp \\left( \\frac{l_r}{\\sigma} \\right) \\right), &\\text{if} \\; \\frac{N - 2r}{\\sigma} - \\lambda \\neq 0 \\\\\n",
    "         \\displaystyle b_{r + 1} \\exp \\left( \\frac{l_r}{\\sigma} \\right), &\\text{if} \\; \\frac{N - 2r}{\\sigma} - \\lambda = 0\n",
    "\\end{cases}\\\\\n",
    "I_{i = r+1, \\ldots, N-1} &= \\begin{cases}\n",
    "         \\frac{\\sigma}{N -2i - \\sigma\\lambda} \\cdot \\left(\\exp \\left( \\frac{b_{i + 1}(N - 2i) + l_i}{\\sigma} - b_{i + 1}\\lambda \\right) - \\exp\\left( \\frac{b_{i}(N - 2i) + l_i}{\\sigma} - b_{i}\\lambda \\right) \\right), &\\text{if} \\; \\frac{N - 2i}{\\sigma} - \\lambda \\neq 0 \\\\\n",
    "         \\displaystyle \\left(b_{i + 1} - b_i \\right) \\exp \\left( \\frac{l_i}{\\sigma} \\right), &\\text{if} \\; \\frac{N - 2i}{\\sigma} - \\lambda = 0\n",
    "     \\end{cases} \\\\\n",
    "I_N &= \\frac{1}{N + \\lambda\\sigma} \\cdot \\exp \\biggl(\\frac{-b_N N + l_N}{\\sigma} - b_N \\lambda \\biggr)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a piecewise defined probability distribution we use the weights of the corresponding integrals to choose with a uniformly distributed random variable $s$ in which part we are sampling. Afterwards we generate a random variable with the fitting shpae on that onterval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sample uniformly distributed $s_1$ and determine smallest $i \\in \\{r, \\ldots, N\\}$ such that $s_1 \\leq \\frac{\\sum_{k = r}^i I_k}{\\sum_{l = r}^N I_l}$\n",
    "\n",
    "2. We now want to sample the according random variable with support inside of $[b_i, b_{i + 1}]$ (with $b_r \\equiv 0$).\n",
    "\n",
    "3. We start by sampling a uniformly distributed random variable $s$ on $[0, 1]$. If $\\frac{N - 2i}/\\sigma - \\lambda = 0$ we can just scale $s$ on $[b_i, b_{i + 1}]$. Else we can use the transformation \n",
    "\\begin{align*}\n",
    "        f(s) \\equiv \\frac{\\log \\left( \\exp \\left( \\left(\\frac{N - 2i}{\\sigma} - \\lambda \\right) \\cdot b_i \\right) + s \\cdot \\left( \\exp \\left( \\left(\\frac{N - 2i}{\\sigma} - \\lambda \\right) \\cdot b_{i + 1} \\right) - \\exp \\left( \\left(\\frac{N - 2i}{\\sigma} - \\lambda \\right) \\cdot b_i \\right) \\right) \\right)}{\\frac{N - 2i}{\\sigma} - \\lambda}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset(data, simulation, shape, lamda):\n",
    "    res = data - simulation\n",
    "    b_vector = np.sort(res)\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    bounds = np.append(np.append(-np.inf, b_vector), np.inf)\n",
    "    r = np.argmax(bounds >= 0)-1\n",
    "    \n",
    "    probability_mass = np.zeros(N + 1 - r)\n",
    "    \n",
    "    #special case i = r\n",
    "    check = (N - 2*r)/shape - lamda\n",
    "    #index choice because we use b_vector not bounds\n",
    "    l_value = np.sum(b_vector[:r]) - np.sum(b_vector[r:])\n",
    "    tmp = l_value/shape\n",
    "    if check != 0:\n",
    "        aux1 = 1 / check\n",
    "        aux2 = (np.exp(tmp + bounds[r+1]*check)-np.exp(tmp))\n",
    "    else:\n",
    "        aux1 = bounds[r + 1]\n",
    "        aux2 = np.exp(tmp)\n",
    "    \n",
    "    probability_mass[0] = aux1 * aux2\n",
    "    \n",
    "    #general case i = r+1, ..., N-1\n",
    "    for n in range(r + 1, N):\n",
    "        #It is sufficient to add the change\n",
    "        l_value += 2 * bounds[n]\n",
    "        tmp = l_value / shape\n",
    "        check = (N - 2*n)/shape - lamda\n",
    "        \n",
    "        if check != 0:\n",
    "            aux1 = 1 / check\n",
    "            aux2 = np.exp(tmp + bounds[n+1]*check) - np.exp(tmp + bounds[n]*check)\n",
    "        else:\n",
    "            aux1 = bounds[n + 1]\n",
    "            aux2 = np.exp(tmp)\n",
    "        \n",
    "        \n",
    "        #here we add up the k_i values\n",
    "        probability_mass[n - r] = probability_mass[n - r - 1] + aux1*aux2\n",
    "        \n",
    "        \n",
    "    #special case i = N\n",
    "    l_value += 2 * bounds[N]\n",
    "    tmp = l_value / shape\n",
    "    aux1 = 1/(N + shape*lamda)\n",
    "    aux2 = np.exp(tmp - bounds[N]*(N/shape + lamda))\n",
    "    \n",
    "    normalisation_constant = probability_mass[N - r -1] + aux1*aux2\n",
    "    \n",
    "    probability_mass = probability_mass / normalisation_constant\n",
    "    probability_mass[N-r] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    s = Generator.uniform(size = 2)\n",
    "    i = np.argmax(probability_mass >= s[0])\n",
    "\n",
    "    factor = (N - 2*(i + r))/shape - lamda\n",
    "    if factor == 0:\n",
    "        if i == 0:\n",
    "            offset = s[1] * bounds[r + 1]\n",
    "        else:\n",
    "            offset = s[1] * (bounds[i + 1 + r] - bounds[i + r])\n",
    "    else:\n",
    "        if i == 0:\n",
    "            lb = 0\n",
    "        else:\n",
    "            lb = bounds[i + r]\n",
    "\n",
    "        if i == N - r:\n",
    "            offset = np.log(1 - s[1])/ factor + lb\n",
    "        else:\n",
    "            ub = bounds[i + 1 + r]\n",
    "            compensate = factor * ub\n",
    "            offset = (compensate + np.log(np.exp(factor*lb - compensate) + s[1] *(np.exp(factor*ub - compensate) - np.exp(factor*lb - compensate)))) / factor\n",
    "            \n",
    "    return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Generator = np.random.default_rng()\n",
    "\n",
    "offset_samples = np.zeros([np.shape(\n",
    "    result.sample_result.trace_x[0, result.sample_result.burn_in:, 0])[0], 1])\n",
    "\n",
    "for index, parameter_sample in enumerate(result.sample_result.trace_x[0, result.sample_result.burn_in:, :]):\n",
    "    shape = parameter_sample[-1]\n",
    "    _simulation = simulate_model(np.power(10, parameter_sample), tvec)\n",
    "    simulation = np.asarray(_simulation)\n",
    "    \n",
    "    offset_samples[index, :] = get_offset(data, simulation, shape, lamda)\n",
    "    \n",
    "if save_results:\n",
    "    results = [result.sample_result, offset_samples]\n",
    "    with open('Results/Offset_marginalized/' + str(n) + '.pickle','wb') as result_file:\n",
    "        pickle.dump(results, result_file, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = sns.kdeplot(offset_samples[:,0], shade=True, color='C0')\n",
    "plt.xlabel('offset')\n",
    "plt.ylabel('kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_MAP = True\n",
    "\n",
    "if plot_MAP:\n",
    "    MAP_index = np.argmax(-result.sample_result.trace_neglogpost[0,result.sample_result.burn_in:])\n",
    "    MAP = result.sample_result.trace_x[0,result.sample_result.burn_in+MAP_index,:]\n",
    "    print(MAP)\n",
    "\n",
    "    tvec_for_plotting = np.linspace(tvec[0],tvec[-1],100)\n",
    "    \n",
    "    scale_MAP = MAP[-1]\n",
    "\n",
    "    # simulate model\n",
    "    _simulation = simulate_model(np.power(10, MAP), tvec)\n",
    "    simulation = np.asarray(_simulation)\n",
    "    \n",
    "    offset_MAP = get_offset(data, simulation, scale_MAP, lamda)\n",
    "    print(offset_MAP)\n",
    "    \n",
    "    _simulation = simulate_model(np.power(10, MAP), tvec_for_plotting)\n",
    "    simulation_for_plotting = np.asarray(offset_MAP + _simulation)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(tvec,data,'or',label='Corrupted data')\n",
    "    plt.plot(tvec_for_plotting,simulation_for_plotting,'k',label='MAP simulation')\n",
    "    plt.xlabel('Time [a.u.]')\n",
    "    plt.ylabel('Signal [a.u.]')\n",
    "    #plt.ylim([0,2])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-5c126572",
   "language": "python",
   "display_name": "PyCharm (pyPESTO)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}