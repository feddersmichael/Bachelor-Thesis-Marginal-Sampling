{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pypesto\n",
    "import pypesto.petab\n",
    "import pypesto.optimize as optimize\n",
    "import pypesto.sample as sample\n",
    "import pypesto.visualize as visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import petab\n",
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "from scipy.special import gamma\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "datatype = \"original\"\n",
    "\n",
    "# import to petab\n",
    "if datatype == \"original\":\n",
    "    petab_problem = petab.Problem.from_yaml(\n",
    "    \"corrupted_data/SS_conversion_reaction_original.yaml\")\n",
    "elif datatype == \"switch\":\n",
    "    petab_problem = petab.Problem.from_yaml(\n",
    "    \"corrupted_data/SS_conversion_reaction_switch.yaml\")\n",
    "elif datatype == \"loss\":\n",
    "    petab_problem = petab.Problem.from_yaml(\n",
    "    \"corrupted_data/SS_conversion_reaction_loss.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def analytical_b(t, a0, b0, k1, k2):\n",
    "    return (k2 - k2 * np.exp(-(k2 + k1) * t)) / (k2 + k1)\n",
    "\n",
    "def simulate_model(x, tvec):\n",
    "    # assign parameters\n",
    "    k1, k2, _ = x\n",
    "    # define initial conditions\n",
    "    a0 = 1\n",
    "    b0 = 0\n",
    "    # simulate model\n",
    "    simulation = [analytical_b(t, a0, b0, k1, k2)\n",
    "                   for t in tvec]\n",
    "    return simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the model, we need to define the objective function. This time we will do it via an external function that will be used then by pyPESTO instead of using the built-in ones.\n",
    "\n",
    "For numerical reasons we will implement the log likelihood and log prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    p(D|\\theta,\\sigma) =& \\frac{\\sigma \\lambda}{(2 \\sigma)^N} \\Biggl( \\frac{1}{N -2r - \\sigma\\lambda} \\cdot \\Biggl(\\exp\\biggl( \\frac{b_{r + 1}(N - 2r) + l_r}{\\sigma} - b_{r + 1}\\lambda \\biggr) - \\exp \\biggl( \\frac{l_r}{\\sigma} \\biggr) \\Biggr) \\\\\n",
    "    &+ \\sum_{i = r + 1}^{N-1} \\frac{1}{N -2i - \\sigma\\lambda} \\cdot \\Biggl(\\exp\\biggl( \\frac{b_{i + 1}(N - 2i) + l_i}{\\sigma} - b_{i + 1}\\lambda \\biggr) - \\exp\\biggl( \\frac{b_{i}(N - 2i) + l_i}{\\sigma} - b_{i}\\lambda \\biggr) \\Biggr) \\\\\n",
    "    &+ \\frac{1}{N + \\sigma\\lambda} \\cdot \\exp \\biggl(\\frac{-b_N N + l_N}{\\sigma} - b_N \\lambda \\biggr) \\Biggr)\n",
    " \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def negative_log_marginalised_likelihood(x):\n",
    "    scale = x[2]\n",
    "    \n",
    "    data = np.asarray(petab_problem.measurement_df.measurement)\n",
    "    tvec = np.asarray(petab_problem.measurement_df.time)\n",
    "    N = len(tvec)\n",
    "    \n",
    "    # simulate model\n",
    "    _simulation = simulate_model(np.exp(x), tvec)\n",
    "    simulation = np.asarray(_simulation)\n",
    "    \n",
    "    # evaluate standard log likelihood\n",
    "    res = data - simulation\n",
    "    b_vector = np.sort(res)\n",
    "    \n",
    "    bounds = np.append(np.append(-np.inf, b_vector), np.inf)\n",
    "    r = np.argmax(bounds >= 0)-1\n",
    "    \n",
    "    marginal_posterior = 0\n",
    "    for n in range(r, N + 1):\n",
    "        l_value = np.sum(b_vector[:n]) - np.sum(b_vector[n:])\n",
    "        tmp = l_value/scale\n",
    "        if n == r:\n",
    "            aux1 = 1/(N-2*r-scale*lamda)\n",
    "            aux2 = (np.exp(tmp+bounds[r+1]*((N-2*r)/scale-lamda))-np.exp(tmp))\n",
    "\n",
    "        elif n == N:\n",
    "            aux1 = 1/(N+scale*lamda)\n",
    "            aux2 = np.exp(tmp+bounds[n]*(-N/scale-lamda))\n",
    "\n",
    "        else:\n",
    "            aux = (N-2*n)/scale-lamda\n",
    "            aux1 = 1/(N-2*n-scale*lamda)\n",
    "            aux2 = np.exp(tmp+bounds[n+1]*aux)-np.exp(tmp+bounds[n]*aux)\n",
    "        \n",
    "        marginal_posterior += aux1*aux2\n",
    "        \n",
    "    log_marginal_posterior = np.log(marginal_posterior)\n",
    "    log_marginal_posterior += -N*(np.log(2) + np.log(scale)) + np.log(lamda) + np.log(scale) \n",
    "    \n",
    "    return -log_marginal_posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the objective function defined, we need to create a pyPESTO problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def marginal_sampling():\n",
    "    \"\"\"Creates a pyPESTO problem.\"\"\"\n",
    "    objective = pypesto.Objective(fun=negative_log_marginalised_likelihood)\n",
    "    problem = pypesto.Problem(objective=objective,  # objective function\n",
    "                              lb=[-5, -5, 0],  # lower bounds\n",
    "                              ub=[5, 5, np.inf],  # upper bounds\n",
    "                              x_names=['k1', 'k2', 'scale'],  # parameter names\n",
    "                              x_scales=['log', 'log', 'lin'])  # parameter scale\n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Prior dependent paramters\n",
    "lamda = 0.01\n",
    "\n",
    "# create the estimation problem\n",
    "problem = marginal_sampling()\n",
    "\n",
    "# MCMC chain length\n",
    "n_samples= 1e5\n",
    "\n",
    "# call the sampler of choice\n",
    "sampler = sample.AdaptiveMetropolisSampler()\n",
    "\n",
    "x0=np.array([-1.2741, -0.6160, 0.3684])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform the actual sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 11145/100000 [00:10<01:23, 1069.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-7435209fce0a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;31m# perform MCMC sampling\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     result = sample.sample(problem, n_samples=n_samples, sampler=sampler,\n\u001B[0m\u001B[0;32m     13\u001B[0m                            x0=x0)\n\u001B[0;32m     14\u001B[0m     \u001B[1;31m# calculate effective sample size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\sample\\sample.py\u001B[0m in \u001B[0;36msample\u001B[1;34m(problem, n_samples, sampler, x0, result)\u001B[0m\n\u001B[0;32m     66\u001B[0m     \u001B[1;31m# perform the sampling and track time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m     \u001B[0mt_start\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_time\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 68\u001B[1;33m     \u001B[0msampler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_samples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mn_samples\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     69\u001B[0m     \u001B[0mt_elapsed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_time\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mt_start\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m     \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Elapsed time: \"\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt_elapsed\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\sample\\metropolis.py\u001B[0m in \u001B[0;36msample\u001B[1;34m(self, n_samples, beta)\u001B[0m\n\u001B[0;32m     55\u001B[0m             \u001B[0mi\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     56\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;36m1000\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 57\u001B[1;33m                 \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     58\u001B[0m             \u001B[1;31m# perform step\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m             x, lpost, lprior = self._perform_step(\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\sample\\metropolis.py\u001B[0m in \u001B[0;36m_perform_step\u001B[1;34m(self, x, lpost, lprior, beta)\u001B[0m\n\u001B[0;32m     74\u001B[0m         \u001B[0maccept\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m         \"\"\"\n\u001B[1;32m---> 76\u001B[1;33m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     77\u001B[0m         \u001B[1;31m# propose step\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     78\u001B[0m         \u001B[0mx_new\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndarray\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_propose_parameter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\envs\\Bachelor-Thesis\\lib\\site-packages\\pypesto\\sample\\adaptive_metropolis.py\u001B[0m in \u001B[0;36m_propose_parameter\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_propose_parameter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 60\u001B[1;33m         \u001B[0mx_new\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmultivariate_normal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_cov\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     61\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mx_new\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mmtrand.pyx\u001B[0m in \u001B[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Define number of runs\n",
    "runs = 1\n",
    "\n",
    "save_results = False # for testing just set to False\n",
    "\n",
    "# Loop over n runs\n",
    "for n in range(runs):\n",
    "    # set initial random seed\n",
    "    np.random.seed(1)\n",
    "    # perform MCMC sampling\n",
    "    result = sample.sample(problem, n_samples=n_samples, sampler=sampler,\n",
    "                           x0=x0)\n",
    "    # calculate effective sample size\n",
    "    sample.effective_sample_size(result=result)\n",
    "\n",
    "    # save the results as a pickle object\n",
    "    if save_results:\n",
    "        results = [result.sample_result]\n",
    "        with open('Results/Offset_marginalized/' + str(n) + '.pickle','wb') as result_file:\n",
    "            pickle.dump(results, result_file, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some built-in visualization functions that one can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = visualize.sampling_fval_traces(result,full_trace=True)\n",
    "# Visualize the parameter trace\n",
    "ax = visualize.sampling.sampling_parameter_traces(result, use_problem_bounds=False,\n",
    "                                                  full_trace=True, size=(12,5))\n",
    "# Visualize the one-dimensional marginals --> Important!\n",
    "ax = visualize.sampling_1d_marginals(result, size=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to sample the offset $c$ from our generated data. It is distributed as a piecewise exponential distribution with this density function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\mathbf{1}_{[0, b_{r + 1})}(c) \\exp\\biggl( c \\cdot \\biggl( \\frac{N - 2r}{\\sigma} - \\lambda \\biggr) + \\frac{l_r}{\\sigma} \\biggr) + \\sum_{i = r + 1}^N \\mathbf{1}_{[b_i, b_{i + 1})}(c) \\exp \\biggl( c \\cdot \\biggl( \\frac{N - 2i}{\\sigma} - \\lambda \\biggr) + \\frac{l_i}{\\sigma} \\biggr)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample this distribution we also need the mass of the corresponding integral over every section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "I_r &= \\frac{1}{N -2r - \\lambda\\sigma} \\cdot \\Biggl(\\exp\\biggl( \\frac{b_{r + 1}(N - 2r) + l_r}{\\sigma} - b_{r + 1}\\lambda \\biggr) - \\exp \\biggl( \\frac{l_r}{\\sigma} \\biggr) \\Biggr)\\\\\n",
    "I_{i = r+1, \\ldots, N-1} &= \\frac{1}{N -2i - \\lambda\\sigma} \\cdot \\Biggl(\\exp\\biggl( \\frac{b_{i + 1}(N - 2i) + l_i}{\\sigma} - b_{i + 1}\\lambda \\biggr) - \\exp\\biggl( \\frac{b_{i}(N - 2i) + l_i}{\\sigma} - b_{i}\\lambda \\biggr) \\Biggr) \\\\\n",
    "I_N &= \\frac{1}{N + \\lambda\\sigma} \\cdot \\exp \\biggl(\\frac{-b_N N + l_N}{\\sigma} - b_N \\lambda \\biggr)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a piecewise defined probability distribution we use the weights of the corresponding integrals to choose with a uniformly distributed random variable $s$ in which part we are sampling. Afterwards we generate an exponentially distributed random variable which has only values in that interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sample uniformly distributed $s_1$ and determine smallest $i \\in \\{r, \\ldots, N\\}$ such that $s_1 \\leq \\frac{\\sum_{k = r}^i I_k}{\\sum_{l = r}^N I_l}$\n",
    "\n",
    "2. We now want to sample an exponential distributed random variable but only inside of $[b_{i-1}, b_i]$ (with $b_{r-1} \\equiv 0$).\n",
    "\n",
    "3. For this we can use the trasnformation $b_{i - 1} - C_i \\log\\left(E_i + (1-E_I) s_2 \\right)$ Hereby is $s_2$ a different uniformly distributed random variable. $C_i$ is the shape parameter of the corresponding exponential distribution. $b_{i - 1}$ is in the code noted as staring value.\n",
    "\n",
    "4. $E_i$ is later noted as the edge value and is necessary to assure that we only sample the exponential distribution inside the borders. It is defined as $E_i \\equiv \\exp\\left( - C_i \\left( b_i - b_{i - 1} \\right) \\right)$\n",
    "\n",
    "I oriented on the following response https://math.stackexchange.com/questions/3512581/generating-samples-from-a-piecewise-probability-density-function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_offset(data, simulation, scale, lamda):\n",
    "    res = data - simulation\n",
    "    b_vector = np.sort(res)\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    bounds = np.append(np.append(-np.inf, b_vector), np.inf)\n",
    "    r = np.argmax(bounds >= 0)-1\n",
    "    \n",
    "    probability_mass = np.zeros(N-r+1)\n",
    "    normalisation_constant = 0\n",
    "\n",
    "    \n",
    "    for n in range(r, N + 1):\n",
    "        l_value = np.sum(b_vector[:n]) - np.sum(b_vector[n:])\n",
    "        tmp = l_value/scale\n",
    "        if n == r:\n",
    "            aux1 = 1/(N-2*r-scale*lamda)\n",
    "            aux2 = (np.exp(tmp+bounds[r+1]*((N-2*r)/scale-lamda))-np.exp(tmp))\n",
    "            probability_mass[0] = aux1 * aux2\n",
    "\n",
    "        elif n == N:\n",
    "            aux1 = 1/(N+scale*lamda)\n",
    "            aux2 = np.exp(tmp+bounds[n]*(-N/scale-lamda))\n",
    "            normalisation_constant = probability_mass[n-r-1] + aux1 * aux2\n",
    "\n",
    "        else:\n",
    "            aux = (N-2*n)/scale-lamda\n",
    "            aux1 = 1/(N-2*n-scale*lamda)\n",
    "            aux2 = np.exp(tmp+bounds[n+1]*aux)-np.exp(tmp+bounds[n]*aux)\n",
    "            probability_mass[n - r] = probability_mass[n-r-1] + aux1 * aux2\n",
    "            \n",
    "    \n",
    "    probability_mass = probability_mass / normalisation_constant\n",
    "    probability_mass[N-r] = 1\n",
    "    \n",
    "    s = Generator.uniform(size = 2)\n",
    "    i = np.argmax(probability_mass > s[0])\n",
    "    distr_val = (N - 2*i)/scale - lamda\n",
    "    \n",
    "    if i == 0:\n",
    "        edge_value = np.exp(-distr_val * bounds[r + 1])\n",
    "        start_value = bounds[r + 1]\n",
    "        \n",
    "    elif i == N-r:\n",
    "        edge_value = 0\n",
    "        start_value = bounds[N]\n",
    "        \n",
    "    elif distr_val > 0:\n",
    "        edge_value = np.exp(-distr_val * (bounds[i + 1] - bounds[i]))\n",
    "        start_value = bounds[i + r + 1]\n",
    "            \n",
    "    else:\n",
    "        edge_value = np.exp(-distr_val * (bounds[i + 1] - bounds[i]))\n",
    "        start_value = bounds[i + r]\n",
    "        \n",
    "    if distr_val > 0:\n",
    "        offset = start_value + np.log(edge_value + (1 - edge_value) * s[1])/distr_val\n",
    "        \n",
    "    else:\n",
    "        offset = start_value - np.log(edge_value + (1 - edge_value) * s[1])/distr_val\n",
    "    \n",
    "    return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tvec = np.asarray(petab_problem.measurement_df.time)\n",
    "data = np.asarray(petab_problem.measurement_df.measurement)\n",
    "\n",
    "Generator = np.random.default_rng()\n",
    "\n",
    "offset_samples = np.zeros([np.shape(\n",
    "    result.sample_result.trace_x[0, result.sample_result.burn_in:, 0])[0], 1])\n",
    "\n",
    "for index, parameter_sample in enumerate(result.sample_result.trace_x[0, result.sample_result.burn_in:, :]):\n",
    "    scale = parameter_sample[-1]\n",
    "    _simulation = simulate_model(np.exp(parameter_sample), tvec)\n",
    "    simulation = np.asarray(_simulation)\n",
    "    \n",
    "    offset_samples[index, :] = get_offset(data, simulation, scale, lamda)\n",
    "    \n",
    "if save_results:\n",
    "    results = [result.sample_result, offset_samples]\n",
    "    with open('Results/Offset_marginalized/' + str(n) + '.pickle','wb') as result_file:\n",
    "        pickle.dump(results, result_file, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = sns.kdeplot(offset_samples[:,0], shade=True, color='C0')\n",
    "plt.xlabel('offset')\n",
    "plt.ylabel('kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_MAP = True\n",
    "\n",
    "if plot_MAP:\n",
    "    MAP_index=np.argmax(-result.sample_result.trace_neglogpost[0,result.sample_result.burn_in:])\n",
    "    MAP = result.sample_result.trace_x[0,result.sample_result.burn_in+MAP_index,:]\n",
    "    \n",
    "    # experimental data\n",
    "    data = np.asarray(petab_problem.measurement_df.measurement)\n",
    "    # time vector\n",
    "    tvec = np.asarray(petab_problem.measurement_df.time)\n",
    "\n",
    "    tvec_for_plotting = np.linspace(tvec[0],tvec[-1],100)\n",
    "    \n",
    "    scale_MAP = MAP[-1]\n",
    "\n",
    "    # simulate model\n",
    "    _simulation = simulate_model(np.exp(MAP), tvec)\n",
    "    simulation = np.asarray(_simulation)\n",
    "    \n",
    "    offset_MAP = get_offset(data, simulation, scale_MAP, lamda)\n",
    "    \n",
    "    _simulation = simulate_model(np.exp(MAP), tvec_for_plotting)\n",
    "    simulation_for_plotting = np.asarray(offset_MAP + _simulation)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(tvec,data,'or',label='Corrupted data')\n",
    "    plt.plot(tvec_for_plotting,simulation_for_plotting,'k',label='MAP simulation')\n",
    "    plt.xlabel('Time [a.u.]')\n",
    "    plt.ylabel('Signal [a.u.]')\n",
    "    plt.ylim([0,2])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}